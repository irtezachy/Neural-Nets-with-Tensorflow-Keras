{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intro to Neural Nets.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL5y5fY9Jy_x"
      },
      "source": [
        "#Neural Nets\n",
        "\n",
        "Build a deep neural network to perform more sophisticated linear regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RDY3EeAluPd"
      },
      "source": [
        "## Learning Objectives:\n",
        "  * Create a baseline model with linear regession\n",
        "  * Create a simple deep neural network to compare results.\n",
        "  * Regularize the deep neural network using,\n",
        "    * L1 Regularization\n",
        "    * L2 Regularization\n",
        "    * Dropout Regularization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lM75uNH-sTv2"
      },
      "source": [
        "#Run on TensorFlow 2.x\n",
        "%tensorflow_version 2.x\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n9_cTveKmse"
      },
      "source": [
        "#Import relevant modules\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# The following lines adjust the granularity of reporting. \n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = \"{:.1f}\".format"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_TaJhU4KcuY"
      },
      "source": [
        "## Load the dataset\n",
        "\n",
        "This exercise uses the California Housing Dataset. \n",
        "\n",
        "* `train_df`, which contains the training set\n",
        "* `test_df`, which contains the test set\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZlvdpyYKx7V"
      },
      "source": [
        "train_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\")\n",
        "\n",
        "#shuffle the examples\n",
        "train_df = train_df.reindex(np.random.permutation(train_df.index)) \n",
        "test_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ldP-5z1B2vL"
      },
      "source": [
        "## Normalize values\n",
        "\n",
        "When building a model with multiple features, the values of each feature should cover roughly the same range.  The following code cell normalizes datasets by converting each raw value to its Z-score. (For more information about Z-scores, see the Classification exercise.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMuXLTF87aGT"
      },
      "source": [
        "train_df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8HC-TDgB1D1"
      },
      "source": [
        "#Convert raw values to their Z-scores \n",
        "\n",
        "# Calculate the Z-scores of each column in the training set:\n",
        "train_df_mean = train_df.mean()\n",
        "train_df_std = train_df.std()\n",
        "train_df_norm = (train_df - train_df_mean)/train_df_std\n",
        "\n",
        "# Calculate the Z-scores of each column in the test set.\n",
        "test_df_mean = test_df.mean()\n",
        "test_df_std = test_df.std()\n",
        "test_df_norm = (test_df - test_df_mean)/test_df_std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aX1asXoR7qs3"
      },
      "source": [
        "#train_df_norm.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUtmmXfj9kdB"
      },
      "source": [
        "#plot histograms. Use ; to avoid printing the text\n",
        "pd.plotting.scatter_matrix(train_df_norm, figsize=(15,15));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9ehCgIRjTxy"
      },
      "source": [
        "## Feature Engineering\n",
        "\n",
        "Create 1 feature `latitude` X `longitude` (a feature cross)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ke_CJ3iJ9gCA"
      },
      "source": [
        "train_df_norm['longxlat'] = train_df_norm['longitude']*train_df_norm['latitude']\n",
        "train_features = train_df_norm[['population','median_income','longxlat','median_house_value']]\n",
        "train_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak_TMAzGOIFq"
      },
      "source": [
        "## Build a linear regression model as a baseline\n",
        "\n",
        "Before creating a deep neural net, let's find a baseline loss by running a simple linear regression model that uses the feature layer we just created. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHLRgtRm8IYP"
      },
      "source": [
        "#Instantiate the model\n",
        "model = None\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(units=1, input_shape=(3,)))\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.01), loss=\"mean_squared_error\", metrics=[tf.keras.metrics.MeanSquaredError()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVI82Vp_84bp"
      },
      "source": [
        "#Train the model\n",
        "history = model.fit(x=train_features.iloc[:,0:3], y=train_features['median_house_value'], verbose = 0, batch_size=1000, epochs=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYaXIIDe_T0f"
      },
      "source": [
        "#Plot the loss curve\n",
        "plt.figure()\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Mean Squared Error\")\n",
        "\n",
        "plt.plot(history.history['mean_squared_error'], label='Training Loss')\n",
        "print(\"Training Loss is:\", history.history['loss'][-1])\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYCEJdL-KjQk"
      },
      "source": [
        "## Evaluate the model performance on ther test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcUGLvcCKjms"
      },
      "source": [
        "test_df_norm['longxlat'] = test_df_norm['longitude']*test_df_norm['latitude']\n",
        "test_features = test_df_norm[['population','median_income','longxlat','median_house_value']]\n",
        "#test_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBxBfpSgLDTb"
      },
      "source": [
        "results = model.evaluate(x=test_features.iloc[:,0:3], y=test_features['median_house_value'],verbose = 0, batch_size=300)\n",
        "print(\"Test loss is:\", results[0])\n",
        "print(\"Test MSE is:\", results[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDsCxTREL_zQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3014ezH3C7jT"
      },
      "source": [
        "##Build and train a deep neural net model\n",
        "\n",
        "The `create_model` function defines the topography of the deep neural net, specifying the following:\n",
        "\n",
        "* The number of layers in the deep neural net.\n",
        "* The number of nodesin each layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_QnruagvcaK"
      },
      "source": [
        "#Instantiate the model\n",
        "nnmodel = None\n",
        "nnmodel = tf.keras.Sequential()\n",
        "\n",
        "#Define the first hidden layer with 20 nodes.   \n",
        "nnmodel.add(tf.keras.layers.Dense(units=20, activation='relu', name='Hidden1'))\n",
        "\n",
        "#Define the second hidden layer with 12 nodes. \n",
        "nnmodel.add(tf.keras.layers.Dense(units=12, activation='relu', name='Hidden2'))\n",
        "  \n",
        "#Define the output layer.\n",
        "nnmodel.add(tf.keras.layers.Dense(units=1, name='Output')) \n",
        "\n",
        "nnmodel.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01),\n",
        "                loss=\"mean_squared_error\",\n",
        "                metrics=[tf.keras.metrics.MeanSquaredError()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYm6dILZxSds"
      },
      "source": [
        "#Train the model\n",
        "nnhistory = nnmodel.fit(x=train_features.iloc[:,0:3], y=train_features['median_house_value'], verbose = 0, batch_size=1000, epochs=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejzVXjsXxymx"
      },
      "source": [
        "#Plot the loss curve\n",
        "plt.figure()\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Mean Squared Error\")\n",
        "\n",
        "plt.plot(nnhistory.history['mean_squared_error'], label='Training Loss')\n",
        "print(\"Training Loss is:\", nnhistory.history['loss'][-1])\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37gObectyI6y"
      },
      "source": [
        "#Test the model\n",
        "nnresults = nnmodel.evaluate(x=test_features.iloc[:,0:3], y=test_features['median_house_value'],verbose = 0, batch_size=300)\n",
        "print(\"Test loss is:\", nnresults[0])\n",
        "print(\"Test MSE is:\", nnresults[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlPXK-SmmjQ2"
      },
      "source": [
        "##Compare the two models\n",
        "\n",
        "The training loss of the deep neural network model (0.47) was consistently lower than that of the linear regression model (0.51), which suggests that the deep neural network model will make better predictions than the linear regression model. Performance can be improved by optimizing the neural network by adding more layers and nodes in the network.\n",
        "\n",
        "However, the model's loss against the test set is **still higher** than the loss against the training set.  In other words, the deep neural network is overfitting to the data in the training set.  To reduce overfitting, regularize the model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu7R_ZpDopIj"
      },
      "source": [
        "##Regularize the deep neural network\n",
        "\n",
        "\n",
        "  * L1 regularization\n",
        "  * L2 regularization\n",
        "  * Dropout regularization\n",
        "\n",
        "Experiment with one or more regularization mechanisms to bring the test loss closer to the training loss (while still keeping test loss relatively low).  \n",
        "\n",
        "**Note:** When you add a regularization function to a model, you might need to tweak other hyperparameters. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8qEUhQkE6S8"
      },
      "source": [
        "### Implementing L1 or L2 regularization\n",
        "\n",
        "To use L1 or L2 regularization on a hidden layer, specify the `kernel_regularizer` argument to [tf.keras.layers.Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense). Assign one of the following methods to this argument:\n",
        "\n",
        "* `tf.keras.regularizers.l1` for L1 regularization\n",
        "* `tf.keras.regularizers.l2` for L2 regularization\n",
        "\n",
        "Each of the preceding methods takes an `l` parameter, which adjusts the [regularization rate](https://developers.google.com/machine-learning/glossary/#regularization_rate). Assign a decimal value between 0 and 1.0 to `l`; the higher the decimal, the greater the regularization. For example, the following applies L2 regularization at a strength of 0.05. \n",
        "\n",
        "```\n",
        "model.add(tf.keras.layers.Dense(units=20, \n",
        "                                activation='relu',\n",
        "                                kernel_regularizer=tf.keras.regularizers.l2(l=0.01),\n",
        "                                name='Hidden1'))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnwEQ-Zo1tYG"
      },
      "source": [
        "#Instantiate the model\n",
        "l1model = None\n",
        "l1model = tf.keras.Sequential()\n",
        "\n",
        "#Define the first hidden layer with 20 nodes.   \n",
        "l1model.add(tf.keras.layers.Dense(units=20, activation='relu', name='Hidden1', kernel_regularizer=tf.keras.regularizers.l1(0.04)))\n",
        "\n",
        "#Define the second hidden layer with 12 nodes. \n",
        "l1model.add(tf.keras.layers.Dense(units=12, activation='relu', name='Hidden2', kernel_regularizer=tf.keras.regularizers.l1(0.04)))\n",
        "  \n",
        "#Define the output layer.\n",
        "l1model.add(tf.keras.layers.Dense(units=1, name='Output')) \n",
        "\n",
        "l1model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01),\n",
        "                loss=\"mean_squared_error\",\n",
        "                metrics=[tf.keras.metrics.MeanSquaredError()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyu4Qtje2JPQ"
      },
      "source": [
        "#Train the model\n",
        "l1history = l1model.fit(x=train_features.iloc[:,0:3], y=train_features['median_house_value'], verbose = 0, batch_size=1000, epochs=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVMNzzTp2OI3"
      },
      "source": [
        "#Plot the loss curve\n",
        "plt.figure()\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Mean Squared Error\")\n",
        "\n",
        "plt.plot(l1history.history['mean_squared_error'], label='Training Loss')\n",
        "print(\"Training Loss is:\", l1history.history['loss'][-1])\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seTmvJaR2XPE"
      },
      "source": [
        "#Test the model\n",
        "l1results = l1model.evaluate(x=test_features.iloc[:,0:3], y=test_features['median_house_value'],verbose = 0, batch_size=300)\n",
        "print(\"Test loss is:\", l1results[0])\n",
        "print(\"Test MSE is:\", l1results[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RW894Kio22KW"
      },
      "source": [
        "The training loss and test loss are now closer thus preventing overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3sJ1rSsE_nS"
      },
      "source": [
        "### Implementing Dropout regularization\n",
        "\n",
        "You implement dropout regularization as a separate layer in the topography. For example, the following code demonstrates how to add a dropout regularization layer between the first hidden layer and the second hidden layer:\n",
        "\n",
        "```\n",
        "model.add(tf.keras.layers.Dense( *define first hidden layer*)\n",
        " \n",
        "model.add(tf.keras.layers.Dropout(rate=0.25))\n",
        "\n",
        "model.add(tf.keras.layers.Dense( *define second hidden layer*)\n",
        "```\n",
        "\n",
        "The `rate` parameter to [tf.keras.layers.Dropout](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout) specifies the fraction of nodes that the model should drop out during training. "
      ]
    }
  ]
}