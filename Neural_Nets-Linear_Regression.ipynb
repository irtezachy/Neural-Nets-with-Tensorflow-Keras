{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intro to Neural Nets.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL5y5fY9Jy_x"
      },
      "source": [
        "#Neural Nets\n",
        "\n",
        "Build a deep neural network to perform more sophisticated linear regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RDY3EeAluPd"
      },
      "source": [
        "## Learning Objectives:\n",
        "\n",
        "  * Create a simple deep neural network.\n",
        "  * Tune the hyperparameters for a simple deep neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lM75uNH-sTv2"
      },
      "source": [
        "#Run on TensorFlow 2.x\n",
        "%tensorflow_version 2.x\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n9_cTveKmse"
      },
      "source": [
        "#Import relevant modules\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# The following lines adjust the granularity of reporting. \n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = \"{:.1f}\".format"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_TaJhU4KcuY"
      },
      "source": [
        "## Load the dataset\n",
        "\n",
        "This exercise uses the California Housing Dataset. \n",
        "\n",
        "* `train_df`, which contains the training set\n",
        "* `test_df`, which contains the test set\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZlvdpyYKx7V"
      },
      "source": [
        "train_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\")\n",
        "\n",
        "#shuffle the examples\n",
        "train_df = train_df.reindex(np.random.permutation(train_df.index)) \n",
        "test_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ldP-5z1B2vL"
      },
      "source": [
        "## Normalize values\n",
        "\n",
        "When building a model with multiple features, the values of each feature should cover roughly the same range.  The following code cell normalizes datasets by converting each raw value to its Z-score. (For more information about Z-scores, see the Classification exercise.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMuXLTF87aGT"
      },
      "source": [
        "train_df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8HC-TDgB1D1"
      },
      "source": [
        "#Convert raw values to their Z-scores \n",
        "\n",
        "# Calculate the Z-scores of each column in the training set:\n",
        "train_df_mean = train_df.mean()\n",
        "train_df_std = train_df.std()\n",
        "train_df_norm = (train_df - train_df_mean)/train_df_std\n",
        "\n",
        "# Calculate the Z-scores of each column in the test set.\n",
        "test_df_mean = test_df.mean()\n",
        "test_df_std = test_df.std()\n",
        "test_df_norm = (test_df - test_df_mean)/test_df_std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aX1asXoR7qs3"
      },
      "source": [
        "#train_df_norm.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUtmmXfj9kdB"
      },
      "source": [
        "#plot histograms. Use ; to avoid printing the text\n",
        "pd.plotting.scatter_matrix(train_df_norm, figsize=(15,15));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9ehCgIRjTxy"
      },
      "source": [
        "## Feature Engineering\n",
        "\n",
        "Create 1 feature `latitude` X `longitude` (a feature cross)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ke_CJ3iJ9gCA"
      },
      "source": [
        "train_df_norm['longxlat'] = train_df_norm['longitude']*train_df_norm['latitude']\n",
        "train_features = train_df_norm[['population','median_income','longxlat','median_house_value']]\n",
        "train_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak_TMAzGOIFq"
      },
      "source": [
        "## Build a linear regression model as a baseline\n",
        "\n",
        "Before creating a deep neural net, let's find a baseline loss by running a simple linear regression model that uses the feature layer we just created. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHLRgtRm8IYP"
      },
      "source": [
        "#Instantiate the model\n",
        "model = None\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(units=1, input_shape=(3,)))\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.01), loss=\"mean_squared_error\", metrics=[tf.keras.metrics.MeanSquaredError()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVI82Vp_84bp"
      },
      "source": [
        "#Train the model\n",
        "history = model.fit(x=train_features.iloc[:,0:3], y=train_features['median_house_value'], verbose = 0, batch_size=1000, epochs=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYaXIIDe_T0f"
      },
      "source": [
        "#Plot the loss curve\n",
        "plt.figure()\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Mean Squared Error\")\n",
        "\n",
        "plt.plot(history.history['mean_squared_error'], label='Training Loss')\n",
        "print(\"Training Loss is:\", history.history['loss'][-1])\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYCEJdL-KjQk"
      },
      "source": [
        "## Evaluate the model performance on ther test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcUGLvcCKjms"
      },
      "source": [
        "test_df_norm['longxlat'] = test_df_norm['longitude']*test_df_norm['latitude']\n",
        "test_features = test_df_norm[['population','median_income','longxlat','median_house_value']]\n",
        "#test_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBxBfpSgLDTb"
      },
      "source": [
        "results = model.evaluate(x=test_features.iloc[:,0:3], y=test_features['median_house_value'],verbose = 0, batch_size=300)\n",
        "print(\"Test loss is:\", results[0])\n",
        "print(\"Test MSE is:\", results[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDsCxTREL_zQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f47LmxF5X_pu"
      },
      "source": [
        "Run the following code cell to invoke the the functions defined in the preceding two code cells. (Ignore the warning messages.)\n",
        "\n",
        "**Note:** Because we've scaled all the input data, **including the label**, the resulting loss values will be *much less* than previous models. \n",
        "\n",
        "**Note:** Depending on the version of TensorFlow, running this cell might generate WARNING messages. Please ignore these warnings. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3014ezH3C7jT"
      },
      "source": [
        "## Define a deep neural net model\n",
        "\n",
        "The `create_model` function defines the topography of the deep neural net, specifying the following:\n",
        "\n",
        "* The number of [layers](https://developers.google.com/machine-learning/glossary/#layer) in the deep neural net.\n",
        "* The number of [nodes](https://developers.google.com/machine-learning/glossary/#node) in each layer.\n",
        "\n",
        "The `create_model` function also defines the [activation function](https://developers.google.com/machine-learning/glossary/#activation_function) of each layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pedD5GhlDC-y",
        "cellView": "both"
      },
      "source": [
        "def create_model(my_learning_rate, my_feature_layer):\n",
        "  \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
        "  # Most simple tf.keras models are sequential.\n",
        "  model = tf.keras.models.Sequential()\n",
        "\n",
        "  # Add the layer containing the feature columns to the model.\n",
        "  model.add(my_feature_layer)\n",
        "\n",
        "  # Describe the topography of the model by calling the tf.keras.layers.Dense\n",
        "  # method once for each layer. We've specified the following arguments:\n",
        "  #   * units specifies the number of nodes in this layer.\n",
        "  #   * activation specifies the activation function (Rectified Linear Unit).\n",
        "  #   * name is just a string that can be useful when debugging.\n",
        "\n",
        "  # Define the first hidden layer with 20 nodes.   \n",
        "  model.add(tf.keras.layers.Dense(units=20, \n",
        "                                  activation='relu', \n",
        "                                  name='Hidden1'))\n",
        "  \n",
        "  # Define the second hidden layer with 12 nodes. \n",
        "  model.add(tf.keras.layers.Dense(units=12, \n",
        "                                  activation='relu', \n",
        "                                  name='Hidden2'))\n",
        "  \n",
        "  # Define the output layer.\n",
        "  model.add(tf.keras.layers.Dense(units=1,  \n",
        "                                  name='Output'))                              \n",
        "  \n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(lr=my_learning_rate),\n",
        "                loss=\"mean_squared_error\",\n",
        "                metrics=[tf.keras.metrics.MeanSquaredError()])\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anH4A_yCcZx2"
      },
      "source": [
        "## Define a training function\n",
        "\n",
        "The `train_model` function trains the model from the input features and labels. The [tf.keras.Model.fit](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#fit) method performs the actual training. The `x` parameter of the `fit` method is very flexible, enabling you to pass feature data in a variety of ways. The following implementation passes a Python dictionary in which:\n",
        "\n",
        "* The *keys* are the names of each feature (for example, `longitude`, `latitude`, and so on).\n",
        "* The *value* of each key is a NumPy array containing the values of that feature. \n",
        "\n",
        "**Note:** Although you are passing *every* feature to `model.fit`, most of those values will be ignored. Only the features accessed by `my_feature_layer` will actually be used to train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jv_lJYTcrEF"
      },
      "source": [
        "def train_model(model, dataset, epochs, label_name,\n",
        "                batch_size=None):\n",
        "  \"\"\"Train the model by feeding it data.\"\"\"\n",
        "\n",
        "  # Split the dataset into features and label.\n",
        "  features = {name:np.array(value) for name, value in dataset.items()}\n",
        "  label = np.array(features.pop(label_name))\n",
        "  history = model.fit(x=features, y=label, batch_size=batch_size,\n",
        "                      epochs=epochs, shuffle=True) \n",
        "\n",
        "  # The list of epochs is stored separately from the rest of history.\n",
        "  epochs = history.epoch\n",
        "  \n",
        "  # To track the progression of training, gather a snapshot\n",
        "  # of the model's mean squared error at each epoch. \n",
        "  hist = pd.DataFrame(history.history)\n",
        "  mse = hist[\"mean_squared_error\"]\n",
        "\n",
        "  return epochs, mse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-IXYVfvM4gD"
      },
      "source": [
        "## Call the functions to build and train a deep neural net\n",
        "\n",
        "Okay, it is time to actually train the deep neural net.  If time permits, experiment with the three hyperparameters to see if you can reduce the loss\n",
        "against the test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nj3v5EKQFY8s",
        "cellView": "both"
      },
      "source": [
        "# The following variables are the hyperparameters.\n",
        "learning_rate = 0.01\n",
        "epochs = 20\n",
        "batch_size = 1000\n",
        "\n",
        "# Specify the label\n",
        "label_name = \"median_house_value\"\n",
        "\n",
        "# Establish the model's topography.\n",
        "my_model = create_model(learning_rate, my_feature_layer)\n",
        "\n",
        "# Train the model on the normalized training set. We're passing the entire\n",
        "# normalized training set, but the model will only use the features\n",
        "# defined by the feature_layer.\n",
        "epochs, mse = train_model(my_model, train_df_norm, epochs, \n",
        "                          label_name, batch_size)\n",
        "plot_the_loss_curve(epochs, mse)\n",
        "\n",
        "# After building a model against the training set, test that model\n",
        "# against the test set.\n",
        "test_features = {name:np.array(value) for name, value in test_df_norm.items()}\n",
        "test_label = np.array(test_features.pop(label_name)) # isolate the label\n",
        "print(\"\\n Evaluate the new model against the test set:\")\n",
        "my_model.evaluate(x = test_features, y = test_label, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlPXK-SmmjQ2"
      },
      "source": [
        "## Result: Compare the two models\n",
        "\n",
        "The loss of the deep neural network model (0.36) was consistently lower than that of the linear regression model (0.39), which suggests that the deep neural network model will make better predictions than the linear regression model. Performance can be improved by optimizing the neural network by adding more layers and nodes in the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu7R_ZpDopIj"
      },
      "source": [
        "##Regularize the deep neural network\n",
        "\n",
        "Notice that the model's loss against the test set is *much higher* than the loss against the training set.  In other words, the deep neural network is overfitting to the data in the training set.  To reduce overfitting, regularize the model. \n",
        "\n",
        "  * L1 regularization\n",
        "  * L2 regularization\n",
        "  * Dropout regularization\n",
        "\n",
        "Experiment with one or more regularization mechanisms to bring the test loss closer to the training loss (while still keeping test loss relatively low).  \n",
        "\n",
        "**Note:** When you add a regularization function to a model, you might need to tweak other hyperparameters. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8qEUhQkE6S8"
      },
      "source": [
        "### Implementing L1 or L2 regularization\n",
        "\n",
        "To use L1 or L2 regularization on a hidden layer, specify the `kernel_regularizer` argument to [tf.keras.layers.Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense). Assign one of the following methods to this argument:\n",
        "\n",
        "* `tf.keras.regularizers.l1` for L1 regularization\n",
        "* `tf.keras.regularizers.l2` for L2 regularization\n",
        "\n",
        "Each of the preceding methods takes an `l` parameter, which adjusts the [regularization rate](https://developers.google.com/machine-learning/glossary/#regularization_rate). Assign a decimal value between 0 and 1.0 to `l`; the higher the decimal, the greater the regularization. For example, the following applies L2 regularization at a strength of 0.05. \n",
        "\n",
        "```\n",
        "model.add(tf.keras.layers.Dense(units=20, \n",
        "                                activation='relu',\n",
        "                                kernel_regularizer=tf.keras.regularizers.l2(l=0.01),\n",
        "                                name='Hidden1'))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3sJ1rSsE_nS"
      },
      "source": [
        "### Implementing Dropout regularization\n",
        "\n",
        "You implement dropout regularization as a separate layer in the topography. For example, the following code demonstrates how to add a dropout regularization layer between the first hidden layer and the second hidden layer:\n",
        "\n",
        "```\n",
        "model.add(tf.keras.layers.Dense( *define first hidden layer*)\n",
        " \n",
        "model.add(tf.keras.layers.Dropout(rate=0.25))\n",
        "\n",
        "model.add(tf.keras.layers.Dense( *define second hidden layer*)\n",
        "```\n",
        "\n",
        "The `rate` parameter to [tf.keras.layers.Dropout](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout) specifies the fraction of nodes that the model should drop out during training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tflt9TZEDARW"
      },
      "source": [
        "#@title Double-click for a possible solution\n",
        "\n",
        "# The following \"solution\" uses L2 regularization to bring training loss\n",
        "# and test loss closer to each other. Many, many other solutions are possible.\n",
        "\n",
        "\n",
        "def create_model(my_learning_rate, my_feature_layer):\n",
        "  \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
        "\n",
        "  # Discard any pre-existing version of the model.\n",
        "  model = None\n",
        "\n",
        "  # Most simple tf.keras models are sequential.\n",
        "  model = tf.keras.models.Sequential()\n",
        "\n",
        "  # Add the layer containing the feature columns to the model.\n",
        "  model.add(my_feature_layer)\n",
        "\n",
        "  # Describe the topography of the model. \n",
        "\n",
        "  # Implement L2 regularization in the first hidden layer.\n",
        "  model.add(tf.keras.layers.Dense(units=20, \n",
        "                                  activation='relu',\n",
        "                                  kernel_regularizer=tf.keras.regularizers.l2(0.04),\n",
        "                                  name='Hidden1'))\n",
        "  \n",
        "  # Implement L2 regularization in the second hidden layer.\n",
        "  model.add(tf.keras.layers.Dense(units=12, \n",
        "                                  activation='relu', \n",
        "                                  kernel_regularizer=tf.keras.regularizers.l2(0.04),\n",
        "                                  name='Hidden2'))\n",
        "\n",
        "  # Define the output layer.\n",
        "  model.add(tf.keras.layers.Dense(units=1,  \n",
        "                                  name='Output'))                              \n",
        "  \n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(lr=my_learning_rate),\n",
        "                loss=\"mean_squared_error\",\n",
        "                metrics=[tf.keras.metrics.MeanSquaredError()])\n",
        "\n",
        "  return model     \n",
        "\n",
        "# Call the new create_model function and the other (unchanged) functions.\n",
        "\n",
        "# The following variables are the hyperparameters.\n",
        "learning_rate = 0.007\n",
        "epochs = 140\n",
        "batch_size = 1000\n",
        "\n",
        "label_name = \"median_house_value\"\n",
        "\n",
        "# Establish the model's topography.\n",
        "my_model = create_model(learning_rate, my_feature_layer)\n",
        "\n",
        "# Train the model on the normalized training set.\n",
        "epochs, mse = train_model(my_model, train_df_norm, epochs, \n",
        "                          label_name, batch_size)\n",
        "plot_the_loss_curve(epochs, mse)\n",
        "\n",
        "test_features = {name:np.array(value) for name, value in test_df_norm.items()}\n",
        "test_label = np.array(test_features.pop(label_name)) # isolate the label\n",
        "print(\"\\n Evaluate the new model against the test set:\")\n",
        "my_model.evaluate(x = test_features, y = test_label, batch_size=batch_size) "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}