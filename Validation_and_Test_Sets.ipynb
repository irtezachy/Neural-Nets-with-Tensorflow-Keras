{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Validation_and_Test_Sets.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f3CKqFUqL2-"
      },
      "source": [
        "# Validation Sets and Test Sets\n",
        "In this Colab, we will experiment with validation sets and test sets.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3spZH_kNkWWX"
      },
      "source": [
        "## Learning objectives\n",
        "\n",
        "We will learn the following:\n",
        "\n",
        "  * Split a training set into a smaller training set and a validation set\n",
        "  * Analyze deltas between training set and validation set results.\n",
        "  * Test the trained model with a test set to determine whether your trained model is overfitting.\n",
        "  * Detect and fix a common training problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV82DJO3kWpk"
      },
      "source": [
        "## The dataset\n",
        "\n",
        "We will use the **California Housing Dataset** to predict the `median_house_value`. \n",
        "\n",
        "* The training set is in `california_housing_train.csv`.\n",
        "* The test set is in `california_housing_test.csv`.\n",
        "\n",
        "We will create the validation set by dividing the training set into two parts:\n",
        "\n",
        "* a training set  \n",
        "* a validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBhNIdUatOU6"
      },
      "source": [
        "#Use the right version of TensorFlow\n",
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9D8GgUovHbG0"
      },
      "source": [
        "#Import relevant modules\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = \"{:.1f}\".format"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUnTc_wfd_o3"
      },
      "source": [
        "## Load the datasets\n",
        "train_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\")\n",
        "test_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_KBdj2M_yjM"
      },
      "source": [
        "## Scale the label values\n",
        "\n",
        "The following code cell scales the `median_house_value`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCWrQ5QhDEyp"
      },
      "source": [
        "#train_df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hc7QQhaAFXD"
      },
      "source": [
        "# Scale the training set's label.\n",
        "train_df[\"median_house_value\"] /= 1000\n",
        "\n",
        "# Scale the test set's label\n",
        "test_df[\"median_house_value\"] /= 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQtEEsrHDP0E"
      },
      "source": [
        "#train_df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijwLbrEcSVo3"
      },
      "source": [
        "#Instantiate the model\n",
        "model = None\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(units=1, input_shape=(1,)))\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.08) , loss=\"mean_squared_error\", metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HV-yxoFTuOy"
      },
      "source": [
        "#Train the model\n",
        "history = model.fit(x=train_df['median_income'], y=train_df['median_house_value'], verbose=0, batch_size=100, validation_split=0.2, epochs=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMBUzByiVRhB"
      },
      "source": [
        "#Plot the loss curve\n",
        "plt.figure()\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Root Mean Squared Error\")\n",
        "\n",
        "\n",
        "plt.plot(history.history['root_mean_squared_error'], label=\"Training Loss\")\n",
        "plt.plot(history.history['val_root_mean_squared_error'], label=\"Validation Loss\")\n",
        "plt.legend()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jipBqEQXlsN8"
      },
      "source": [
        "\n",
        "\n",
        "If the data in the training set is similar to the data in the validation set, then the two loss curves and the final loss values should be almost identical. However, the loss curves and final loss values are **not** almost identical. \n",
        "\n",
        "Even if you experiment with different values of `validation_split` it will not fix the problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKa11JK4Pm3f"
      },
      "source": [
        "Evidently, the data in the training set isn't similar enough to the data in the validation set. Because, the original training set is sorted by longitude. Apparently, longitude influences the relationship of total_rooms to median_house_value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnNvkFwwK8WY"
      },
      "source": [
        "# Examine examples 0 through 4 and examples 95 through 99 of the training set\n",
        "train_df.head(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rw4xI1ZEckI8"
      },
      "source": [
        "##Fixing the problem\n",
        "\n",
        "To fix the problem, we need to shuffle the examples in the training set before splitting the examples into a training set and validation set. To do so, add the following line anywhere before calling `train_model`\n",
        "\n",
        "```\n",
        "  shuffled_train_df = train_df.reindex(np.random.permutation(train_df.index))\n",
        "```                                    \n",
        "\n",
        "Pass `shuffled_train_df` (instead of `train_df`) as the second argument to `train_model` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1G7m1aGL6dK"
      },
      "source": [
        "#Instantiate the model\n",
        "model = None\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(units=1, input_shape=(1,)))\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.08) , loss=\"mean_squared_error\", metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_S6AuEElBZ2u"
      },
      "source": [
        "#Train the model\n",
        "#Shuffle the examples, and use 'shuffled_train_df' instead of train_df\n",
        "shuffled_train_df = train_df.reindex(np.random.permutation(train_df.index)) \n",
        "history = model.fit(x=shuffled_train_df['median_income'], y=shuffled_train_df['median_house_value'], verbose=0, batch_size=100, validation_split=0.2, epochs=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WX--RdjWCK5u"
      },
      "source": [
        "#Plot the loss curve\n",
        "plt.figure()\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Root Mean Squared Error\")\n",
        "\n",
        "plt.plot(history.history['root_mean_squared_error'], label=\"Training Loss\")\n",
        "plt.plot(history.history['val_root_mean_squared_error'], label=\"Validation Loss\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PP-O8TOZOeo"
      },
      "source": [
        "## Finaly, evaluate the model performance on the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nd_Sw2cygOip"
      },
      "source": [
        "x_test = test_df['median_income']\n",
        "y_test = test_df['median_house_value']\n",
        "\n",
        "results = model.evaluate(x_test, y_test, batch_size=100)\n",
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoyQKvsjmV_A"
      },
      "source": [
        "Compare the root mean squared error of the model when evaluated on each of the three datasets:\n",
        "\n",
        "* training set, validation set, and test set\n",
        "\n",
        "Ideally, the root mean squared error of all three sets should be similar. **Successfull!**"
      ]
    }
  ]
}